---
title: "dataprep"
author: "Ellie White"
date: "February 22, 2019"
output: html_document
---

A record for data gathering and processing before we hand it over to the modeling algorithm. 

# Contents   
1.0  Basin Boundaries -- Developed from NHDPlusV2
2.0  Climate (Dynamic) -- PRISM  
3.0  Basin Geometry
4.0  Hypsometry -- SRTM
5.0  Soil Properties -- POLARIS  
6.0  Land Cover -- CALVEG
7.0  Geology -- NRCS 
8.0  Unimpaired Flows -- CDEC  
9.0 Month, Season and Year
10.0 Data Prep for modeling       
  
```{r, include=FALSE}
library(knitr)
library(formatR)
opts_chunk$set(
  fig.width  = 7.5,
  fig.height = 7.5,
  collapse   = TRUE,
  tidy       = FALSE
)
```

```{r citations}
# cite R and R Studio
toBibtex(citation())
RStudio.Version()

# cite packages
citethese <- c("raster", "rgdal", "rgeos", "dismo", "geosphere", "prism", "sharpshootR", "reshape2")
for(i in seq_along(citethese)){
  x <- citation(citethese[i])
  print(toBibtex(x))
}

# in case needed
sessionInfo()

remove(i)
remove(x)
remove(citethese)
```

# 1.0 Basin Boundaries -- Developed from NHDPlusV2
What: CDEC basin boundaries developed by joining all HUC 14 levels, or small, sub basins above the outlet point. All processing was done in ArcGIS with the NHDPlusV2 data (i.e., flow direction, small basin boundaries, ...). 
Type (extension): one .shp file that contains all .shp boundaries
Time Resolution: none, static
Modifications: none

```{r basin_data}
library(raster)
# CDEC Basin locations
df <- read.csv("Input Data/CDEC_FNF/station_search.csv", header=TRUE, stringsAsFactors=FALSE, fileEncoding="UTF-8-BOM")

# # uncomment to subset the basins to be just the ones in the sac delta (9)
# df <- df[df$IN_SAC_DELTA==1,]

# keep df as a normal DataFrame for later use, and make spdf a SpatialPolygonsDataFrame
sptdf <- df
coordinates(sptdf) <- ~LONGITUDE + LATITUDE
proj4string(sptdf) <- CRS('+proj=longlat +datum=WGS84')

# basin boundaries
library(rgdal)
basins <- shapefile('Input Data/CDEC_FNF/Catchment_all_daily2.shp')

# projections that are appropriate for California
tealalbers <- crs("+proj=aea +lat_1=34 +lat_2=40.5 +lat_0=0 +lon_0=-120 +x_0=0 +y_0=-4000000 +datum=NAD83 +units=km +ellps=GRS80")
albers <- crs("+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0")

# transform to all to Albers projection
sptdf <- spTransform(sptdf, albers)
basins <- spTransform(basins, albers)

# join the information on the stations to the SpatialPolygonsDataFrame
basins@data <- merge(basins@data, sptdf, by="CDEC_ID")
```

# 2.0 Climate (Dynamic) -- PRISM
What:  
* tmean	Mean temperature, mean(monthly min, monthly max)  
* tmax	Maximum temperature in degrees celcius  
* tmin	Minimum temperature in degrees celcius  
* ppt	Total precipitation (rain and snow) in millimeters  
* vpdmin	Daily minimum vapor pressure deficit [averaged over all days in the month - normal data only]  
* vpdmax	Daily maximum vapor pressure deficit [averaged over all days in the month - normal data only]  
Type (extension): .bil (binary data), gridded rasters for the continental US at 4km resolution  
Time Resolution: 3 different scales available: daily, monthly and 30 year normals. Data is available from 1891 until 2014, however you have to download all data for years prior to 1981. 
Modifications: need to aggregate by basin

```{r prism_data}
library(prism)
# # set the path to download temperature data
# options(prism.path = "Input Data/PRISM_TMP")

# uncomment to download data if not in the directory set above
# # daily data covers 1 Jan 1981 to 30 Jun 2015, I downloaded 2000-01-01 to 2015-06-30
# get_prism_dailys(type = "tmean", minDate = "2003-03-09", maxDate= "2010-01-01", keepZip = FALSE)

# # create a stack of prism files to use for processing later
# prism_tmp <- prism_stack(ls_prism_data()[, 1])

# # set the path to download precipitation data
# options(prism.path = "Input Data/PRISM_PPT")

# # uncomment to download data if not in the directory set above
# get_prism_dailys(type = "ppt", minDate = "2000-01-01", maxDate= "2010-01-01", keepZip = FALSE)

# # create a stack of prism files to use for processing later
# prism_ppt <- prism_stack(ls_prism_data()[, 1])
```

```{r prism_aggregation}
# # aggregate temp and precip rasters by basin boundaries, this function takes a long time and is an "embarrassingly parallel problem", so...
# basins_inc_tmp <- extract(prism_tmp, basins, fun=mean,  weights=FALSE, small=TRUE)
# basins_inc_ppt <- extract(prism_ppt, basins, fun=mean,  weights=FALSE, small=TRUE)

# # try cutting down the processing time with doParallel
# library(foreach)
# library(doParallel)
# 
# # for TMP
# cl <- makeCluster(4)
# registerDoParallel(cl)
# basins_tmp <- foreach(i=1:nrow(basins@data), .combine=rbind) %dopar% {
#   library(raster)
#   extract(prism_tmp, basins[i,], fun=mean, weights=FALSE, small=TRUE)
# }
# stopImplicitCluster()
# 
# # for PPT
# cl <- makeCluster(4)
# registerDoParallel(cl)
# basins_ppt <- foreach(i=1:nrow(basins@data), .combine=rbind) %dopar% {
#   library(raster)
#   extract(prism_ppt, basins[i,], fun=mean, weights=FALSE, small=TRUE)
# }
# stopImplicitCluster()
# 
# # plot to check
# plot(basins_tmp[, 1])
# plot(basins_ppt[, 1])
# 
# # write to a csv file
# write.csv(basins_tmp, file="Input Data/CDEC_FNF/basins_PRISM_TMP.csv", row.names = FALSE)
# write.csv(basins_ppt, file="Input Data/CDEC_FNF/basins_PRISM_PPT.csv", row.names = FALSE)

# read in
basins$TMP <- read.csv("Input Data/CDEC_FNF/basins_PRISM_TMP.csv")
basins$PPT <- read.csv("Input Data/CDEC_FNF/basins_PRISM_PPT.csv")

# fix the column names as actual dates
strspl <- unlist(strsplit(colnames(basins$TMP), split="_"))
date_vector <- strspl[seq(5, length(strspl), 6)]
date_formatted <- paste(substr(date_vector, 1, 4), "-", substr(date_vector, 5, 6), "-", substr(date_vector, 7, 8), sep="")

# since the precip and temp data are for the same time series change the column names on both dataframes
colnames(basins$TMP) <- colnames(basins$PPT) <- date_formatted

# reshape into long format for easy merging later
library(reshape2)
tmp_wide <- basins$TMP
tmp_wide$CDEC_ID <- basins$CDEC_ID
tmp_long <- melt(tmp_wide, id.vars="CDEC_ID", variable.name="DATE", value.name = "TMP")

ppt_wide <- basins$PPT
ppt_wide$CDEC_ID <- basins$CDEC_ID
ppt_long <- melt(ppt_wide, id.vars="CDEC_ID", variable.name="DATE", value.name = "PPT")
```

```{r prism_lags}
# lag-1: lag temp and precip by one month (meaning the previous time steps data will be associated with the current month)
# lag-2: lag temp and precip by two months or lag lag-1 by one month
tmplag1 <- tmplag2 <- tmplag3 <- basins$TMP
colnames(tmplag1) <- date_vector_lag1 <- c(date_formatted[2:length(date_formatted)], NA)
colnames(tmplag2) <- date_vector_lag2 <- c(date_vector_lag1[2:length(date_formatted)], NA)
colnames(tmplag3) <- c(date_vector_lag2[2:length(date_formatted)], NA)

# delete the last day, and last two days, and the last three days, because of the lagging we won't have ppt and tmp complete information
tmplag1_wide <- tmplag1[, 1:(ncol(tmplag1)-1)]
tmplag1_wide$CDEC_ID <- basins$CDEC_ID
tmplag1_long <- melt(tmplag1_wide, id.vars="CDEC_ID", variable.name="DATE", value.name="TMPLAG1")
 
tmplag2_wide <- tmplag2[, 1:(ncol(tmplag2)-2)]
tmplag2_wide$CDEC_ID <- basins$CDEC_ID
tmplag2_long <- melt(tmplag2_wide, id.vars="CDEC_ID", variable.name="DATE", value.name="TMPLAG2")

tmplag3_wide <- tmplag3[, 1:(ncol(tmplag3)-3)]
tmplag3_wide$CDEC_ID <- basins$CDEC_ID
tmplag3_long <- melt(tmplag3_wide, id.vars="CDEC_ID", variable.name="DATE", value.name="TMPLAG3")

# same for ppt
pptlag1 <- pptlag2 <- pptlag3 <- basins$PPT
colnames(pptlag1) <- date_vector_lag1 <- c(date_formatted[2:length(date_formatted)], NA)
colnames(pptlag2) <- date_vector_lag2 <- c(date_vector_lag1[2:length(date_formatted)], NA)
colnames(pptlag3) <- c(date_vector_lag2[2:length(date_formatted)], NA)

pptlag1_wide <- pptlag1[, 1:(ncol(pptlag1)-1)]
pptlag1_wide$CDEC_ID <- basins$CDEC_ID
pptlag1_long <- melt(pptlag1_wide, id.vars="CDEC_ID", variable.name="DATE", value.name="PPTLAG1")
 
pptlag2_wide <- pptlag2[, 1:(ncol(pptlag2)-2)]
pptlag2_wide$CDEC_ID <- basins$CDEC_ID
pptlag2_long <- melt(pptlag2_wide, id.vars="CDEC_ID", variable.name="DATE", value.name="PPTLAG2")

pptlag3_wide <- pptlag3[, 1:(ncol(pptlag3)-3)]
pptlag3_wide$CDEC_ID <- basins$CDEC_ID
pptlag3_long <- melt(pptlag3_wide, id.vars="CDEC_ID", variable.name="DATE", value.name="PPTLAG3")
```

```{r ppt_cumulatives_and_snow}
# will these lags be sufficient to represent ice and snow? No, according to Godsey(2013) in "Effects of changes in winter snowpacks on summer low flows: case studies in the Sierra Nevada, California, USA". They found that "at some locations, low flows exhibit a memory effect in which they depend not only on the current year's snowpack but also on the previous year's snowpack." so do we need to have a cumulative rain that falls as snow (temp under 2degC) or maybe we need to bring in better snow data.

# make a water year function, in California October-September is considered the water year
wateryear <- function(dates, startmonth=10) {
  dates <- as.Date(dates, format="%Y-%m-%d")
  mon <- as.numeric(format(dates,"%m"))
  year <- as.numeric(format(dates,"%Y"))
  offset <- ifelse(mon < startmonth, 0, 1)
  wtyr <- year + offset
}

pptcumul_df <- cbind(tmp_long, PPT=ppt_long$PPT)
pptcumul_df$YEAR <- substring(pptcumul_df$DATE, 1, 4)
pptcumul_df$MONTH <- month.abb[as.integer(substring(pptcumul_df$DATE, 6, 7))]
pptcumul_df$WATERYEAR <- wateryear(pptcumul_df$DATE, 10)
pptcumul_df$PPT_UNDER2 <- ifelse(pptcumul_df$TMP<=2,pptcumul_df$PPT,0)

x <- y <- list() # for ppt and snow for each basin
for (i in unique(pptcumul_df$CDEC_ID)){
  subset <- pptcumul_df[pptcumul_df$CDEC_ID==i,]
  subset <- subset[order(as.Date(subset$DATE, format="%Y-%m-%d")),] # in case it's not ordered, order subset by date
  
  # make empty vectors for ppt and snow for each wateryear
  pptcumul_vect <-  snowcumul_vect <- numeric() 
  for (j in unique(subset$WATERYEAR)){
    subsubset <- subset[subset$WATERYEAR==j,]
    pptcumul <- cumsum(subsubset$PPT)
    pptcumul_vect <- c(pptcumul_vect, pptcumul)
    snowcumul <- cumsum(subsubset$PPT_UNDER2)
    snowcumul_vect <- c(snowcumul_vect, snowcumul)
  }
  x[[i]] <- pptcumul_vect
  y[[i]] <- snowcumul_vect
}

# # coerce list into a data frame, not doing it this way because the lengths are the same so make it a wide dataframe first
# xdf <- data.frame(PPTCUMUL=unlist(x, recursive=TRUE, use.names = TRUE))
# ydf <- data.frame(SNOW=unlist(y, recursive=TRUE, use.names = TRUE))

# check the lengths of the lists
for (n in names(x)){
  print(length(x[[n]]))
}

for (n in names(y)){
  print(length(y[[n]]))
}

# since the lengths of the lists are all the same, make this list a dataframe
xdf <- as.data.frame(x)
xdf$DATE <- date_formatted

ydf <- as.data.frame(y)
ydf$DATE <- date_formatted

pptcumul_df_long <- reshape2:::melt.data.frame(xdf, id.var="DATE", measure.var = 1:(ncol(xdf)-1), variable.name = "CDEC_ID", value.name = "PPTCUMUL")
snowcumul_df_long <- reshape2:::melt.data.frame(ydf, id.var="DATE", measure.var = 1:(ncol(ydf)-1), variable.name = "CDEC_ID", value.name = "SNOW")

# consider making the beginning or record snow for each basin NA, not going to worry about this now

# make the lagged versions
xdf <- data.frame(t(as.data.frame(x)))

pptcumullag1 <- pptcumullag2 <- pptcumullag3 <- xdf
colnames(pptcumullag1) <- date_vector_lag1 <- c(date_formatted[2:length(date_formatted)], NA)
colnames(pptcumullag2) <- date_vector_lag2 <- c(date_vector_lag1[2:length(date_formatted)], NA)
colnames(pptcumullag3) <- c(date_vector_lag2[2:length(date_formatted)], NA)

pptcumullag1_wide <- pptcumullag1[, 1:(ncol(pptcumullag1)-1)]
pptcumullag1_wide$CDEC_ID <- basins$STATION
pptcumullag1_long <- melt(pptcumullag1_wide, id.vars="CDEC_ID", variable.name="DATE", value.name="PPTLAG1")
 
pptcumullag2_wide <- pptcumullag2[, 1:(ncol(pptcumullag2)-2)]
pptcumullag2_wide$CDEC_ID <- basins$STATION
pptcumullag2_long <- melt(pptcumullag2_wide, id.vars="CDEC_ID", variable.name="DATE", value.name="PPTLAG2")

pptcumullag3_wide <- pptcumullag3[, 1:(ncol(pptcumullag3)-3)]
pptcumullag3_wide$CDEC_ID <- basins$STATION
pptcumullag3_long <- melt(pptcumullag3_wide, id.vars="CDEC_ID", variable.name="DATE", value.name="PPTLAG3")

# no longer needed
remove(xdf)
remove(ydf)
remove(x)
remove(y)
```

# 3.0 Basin Geometry
```{r geometry_drainagearea}
library(rgeos)
# units are in map units (m). To convert m^2 to mile^2 divide by 2.59e+6
basins$AREASQM <- raster::area(basins)/2.59e+6
```

```{r geometry_shape}
# shape can be described as circular, rectangular, triangular, or pear. The latter is most common. Shape directly impacts the size of peak discharge and its arrival time at the basin outlet. Peak discharge for a circular basin will arrive sooner than that of an elongate basin of the same area because the tributary network in a circular basin is more compactly organized and tributary flows enter the mainstem at roughly the same time, thus more runoff is delivered to the outlet together, sooner (shorter duration, higher flood peak). L = Length of watershed/ W = Width of watershed

library(dismo)
basins_spldf <- as(basins, "SpatialLinesDataFrame")

# need to make points for each basin separately
basins_name <- unique(basins_spldf$STATION)
basins_length <- basins_width <- basins_shape <-  c()

for (i in 1:length(basins_name)){
  h <- basins_name[i]
  sub_basins <- basins_spldf[basins_spldf$STATION==h, ] 
  sub_sptdf <- spsample(sub_basins, 10000, type="regular")  # sample points on the basin boundary to create points
  rect_hull <- rectHull(sub_sptdf) # to find the length and width of the basin
  rect_coords <- geom(polygons(rect_hull))
  l1 <- pointDistance(rect_coords[1,5:6], rect_coords[2,5:6], lonlat=FALSE)
  l2 <- pointDistance(rect_coords[2,5:6], rect_coords[3,5:6], lonlat=FALSE)
  if(l1 > l2){
    length <- l1
    width <- l2
  } else {
    length <- l2
    width <- l1
  } 
  basins_length <- c(basins_length, length) 
  basins_width <- c(basins_width, width)
  basins_shape <- basins_length/basins_width
}

basins$LENGTH <- basins_length
basins$WIDTH <-  basins_width
basins$SHAPE <- basins_shape
```

```{r geometry_compactness}
# basin compactness = area over perimeter^2*100
library(geosphere)
basins_perimeter <- perimeter(spTransform(basins, CRSobj=CRS("+proj=longlat +datum=NAD83")))
basins$COMPACTNESS <- basins$AREASQM*2.59e+6/(basins_perimeter^2)
```

```{r geometry_drainage_density}
# drainage density, not doing this one
# bring in NHD river network, crop to boundaries, compute length of line segments
```

# 4.0 Hypsometry -- SRTM
What: alt = elevation data from the SRTM 90m model
type (extension): .grd 
time resolution: static  
spacial resolution: 90m (at the equator or 3 arc seconds). The vertical error of the DEMs is reported to be less than 16m. 
note: this data set is split for USA
units: meters
modifications: need to aggregate by basin

```{r hypsometric}
# # uncomment to download in working directory
# elev <- getData('alt', country='USA', mask=TRUE) 

elev <- raster("Input Data/SRTM/USA1_msk_alt.grd")
basins_transformed <- spTransform(basins, crs(elev))

# # uncomment to run again if needed
# basins_mean_elev <- extract(elev, basins_transformed, fun=mean, weights=FALSE, small=TRUE, na.rm=TRUE)
# write.csv(basins_mean_elev, file="Input Data/CDEC_FNF/basins_MEAN_ELEV.csv", row.names = FALSE)
test <- read.csv("Input Data/CDEC_FNF/basins_MEAN_ELEV.csv")
basins$MEANELEV <- test$V1

# gauge elevation: note, this turned out to be close to, but not exactly min elevation. we will use this in liu of min elevation when needed because in theory the lowest elevation should be at the gauge, but the gauge latlons are not quite exact. bad quality data, but what can you do...
sptdf_transformed <- spTransform(sptdf, crs(elev))
sptdf$GAUGEELEV <- extract(elev, sptdf_transformed)

# # relief ratio (Pike and Wilson 1971): the Elevation-Relief Ratio provides hypsometric information about a watershed. = Zavg - Zmin / Zmax - Zmin
# basins_max_elev <- extract(elev, basins_transformed, fun=max, weights=FALSE, small=TRUE, na.rm=TRUE)
# write.csv(basins_max_elev, file="Input Data/CDEC_FNF/basins_MAX_ELEV.csv", row.names = FALSE)
test <- read.csv("Input Data/CDEC_FNF/basins_MAX_ELEV.csv")
basins$MAXELEV <- test$V1

# basin relief ratio: the ratio between total relief (max elev-min elev) and basin length (long axis length)
basins$BASINRELIEFRATIO <- (basins$MAXELEV-sptdf$GAUGEELEV)/basins$LENGTH
  
# basin slope
# center of raster cell that has min or max elevation
# maxmindist <- gDistance()
# basins@data$SLOPE <- (basins@data$MAXELEV-basins@data$MINELEV)/maxmindist
```

Refs to check:
Bedient (1992)
Gray (1970)
Grohmann & Riccomini (2009) Computers & Geosciences 35
Montgomery & Brandon (2002) Earth and Planetary Science Letters 201
Morisawa (1958)
Sarangi et al. (2003)
Sougnez & Vanacker (2011) Hydrology and Earth Systems Sciences 15
Wisler (1959)
Safran et al. (2005) ESPL 30, Fig. 7

# 5.0 Soil Properties -- POLARIS
What: SSURGO processed soil data, 3 arcsec (~100 m)
projection: Lambert Conformal Conic 
Datum: NAD83
url: http://hydrology.cee.duke.edu/POLARIS/PROPERTIES
date retrieved: 05/11/17
type (extension): .tif 
time resolution: static  
spacial resolution: 100 m 
units: meters
modifications: need to aggregate by basin
credit: Nate Chaney

```{r polaris}
fileloc <- "D:/ml with cdec uf - masters/Input Data/POLARIS/"
ksat <- raster(paste0(fileloc, 'ksat_mean_0_5.tif')) # ksat - saturated hydraulic conductivity, cm/hr
silt <- raster(paste0(fileloc, 'silt_mean_0_5.tif')) # silt - silt percentage, %
sand <- raster(paste0(fileloc, 'sand_mean_0_5.tif')) # sand - sand percentage, %
clay <- raster(paste0(fileloc, 'clay_mean_0_5.tif')) # clay - clay percentage, %
slope <- raster(paste0(fileloc, 'slope_mean.tif')) # ??? not explained
awc <- raster(paste0(fileloc, 'awc_mean_0_5.tif')) # awc - available water content, m3/m3
lambda_poresize <- raster(paste0(fileloc, 'lambda_mean_0_5.tif')) # lambda - pore size distribution index (brooks-corey), N/A
n_poresize <- raster(paste0(fileloc, 'n_mean_0_5.tif')) # n - measure of the pore size distribution (van genuchten), N/A
alpha_poresize <- raster(paste0(fileloc, 'alpha_mean_0_5.tif')) # alpha - scale parameter inversely proportional to mean pore diameter (van genuchten), cm-1
resdt <- raster(paste0(fileloc, 'resdt_mean.tif')) # resdt - depth to restriction layer, cm

basins_transformed <- spTransform(basins, crs(ksat))

# # uncomment to extract
# basins_ksat <- extract(ksat, basins_transformed, fun=mean, weights=FALSE, small=TRUE, na.rm=TRUE)
# write.csv(basins_ksat, file="Input Data/CDEC_FNF/basins_KSAT.csv", row.names=FALSE)
test <- read.csv("Input Data/CDEC_FNF/basins_KSAT.csv")
basins@data$KSAT <- test$V1

# basins_silt <- extract(silt, basins_transformed, fun=mean, weights=FALSE, small=TRUE, na.rm=TRUE)
# write.csv(basins_silt, file="Input Data/CDEC_FNF/basins_SILT.csv", row.names=FALSE)
test <- read.csv("Input Data/CDEC_FNF/basins_SILT.csv")
basins@data$SILT <- test$V1

# basins_sand <- extract(sand, basins_transformed, fun=mean, weights=FALSE, small=TRUE, na.rm=TRUE)
# write.csv(basins_sand, file="Input Data/CDEC_FNF/basins_SAND.csv", row.names=FALSE)
test <- read.csv("Input Data/CDEC_FNF/basins_SAND.csv")
basins@data$SAND <- test$V1

# basins_clay <- extract(clay, basins_transformed, fun=mean, weights=FALSE, small=TRUE, na.rm=TRUE)
# write.csv(basins_clay, file="Input Data/CDEC_FNF/basins_CLAY.csv", row.names=FALSE)
test <- read.csv("Input Data/CDEC_FNF/basins_CLAY.csv")
basins@data$CLAY <- test$V1

# check that sand, silt and clay add up to 100, or close enough
basins@data$CLAY+basins@data$SILT+basins@data$SAND

# basins_awc <- extract(awc, basins_transformed, fun=mean, weights=FALSE, small=TRUE, na.rm=TRUE)
# write.csv(basins_awc, file="Input Data/CDEC_FNF/basins_AWC.csv", row.names=FALSE)
test <- read.csv("Input Data/CDEC_FNF/basins_AWC.csv")
basins@data$AWC <- test$V1

# basins_lambda_poresize <- extract(lambda_poresize, basins_transformed, fun=mean, weights=FALSE, small=TRUE, na.rm=TRUE)
# write.csv(basins_lambda_poresize, file="Input Data/CDEC_FNF/basins_LAMBDA.csv", row.names=FALSE)
test <- read.csv("Input Data/CDEC_FNF/basins_LAMBDA.csv")
basins@data$LAMBDA <- test$V1

# basins_n_poresize <- extract(n_poresize, basins_transformed, fun=mean, weights=FALSE, small=TRUE, na.rm=TRUE)
# write.csv(basins_n_poresize, file="Input Data/CDEC_FNF/basins_N.csv", row.names=FALSE)
test <- read.csv("Input Data/CDEC_FNF/basins_N.csv")
basins@data$N <- test$V1

# basins_alpha_poresize <- extract(alpha_poresize, basins_transformed, fun=mean, weights=FALSE, small=TRUE, na.rm=TRUE)
# write.csv(basins_alpha_poresize, file="Input Data/CDEC_FNF/basins_ALPHA.csv", row.names=FALSE)
test <- read.csv("Input Data/CDEC_FNF/basins_ALPHA.csv")
basins@data$N <- test$V1

# basins_resdt <- extract(resdt, basins_transformed, fun=mean, weights=FALSE, small=TRUE, na.rm=TRUE)
# write.csv(basins_resdt, file="Input Data/CDEC_FNF/basins_RESDT.csv", row.names=FALSE)
test <- read.csv("Input Data/CDEC_FNF/basins_RESDT.csv")
basins@data$RESDT <- test$V1

# some more variables that you could consider adding
# soil by hydrologic group A to D
# mean permeability
# mean water capacity
# mean bulk density
# mean organic matter
# mean soil thickness
# mean percent fine and coarse soils
# mean soil erodibility factor (from Universal Soil Loss Equation)
# mean runoff factor (from Universal Soil Loss Equation)
```

# 6.0 Land Cover -- CALVEG
```{r calveg}
# did all of this in arcmap, was easier. keep in case needed
# calveg_cc <- raster('D:/ml with cdec uf - masters/Input Data/CALVEG/cc')
# calveg_cv <- raster('D:/ml with cdec uf - masters/Input Data/CALVEG/cv')
# calveg_gb <- raster('D:/ml with cdec uf - masters/Input Data/CALVEG/gb')
# calveg_nce <- raster('D:/ml with cdec uf - masters/Input Data/CALVEG/nce')
# calveg_ncm <- raster('D:/ml with cdec uf - masters/Input Data/CALVEG/ncm')
# calveg_ncw <- raster('D:/ml with cdec uf - masters/Input Data/CALVEG/ncw')
# calveg_ni <- raster('D:/ml with cdec uf - masters/Input Data/CALVEG/ni')
# calveg_ns <- raster('D:/ml with cdec uf - masters/Input Data/CALVEG/ns')
# calveg_sc <- raster('D:/ml with cdec uf - masters/Input Data/CALVEG/sc')
# calveg_si <- raster('D:/ml with cdec uf - masters/Input Data/CALVEG/si')
# calveg_ss <- raster('D:/ml with cdec uf - masters/Input Data/CALVEG/ss')
# 
# calveg_list <- list(calveg_cc, calveg_cv, calveg_gb, calveg_nce, calveg_ncm, calveg_ncw, calveg_ni, calveg_ns, calveg_sc, calveg_si, calveg_ss)
# 
# lapply(calveg_list, origin)
# rasterOptions(tolerance = 0.1) # if you get different origin error
# .Machine$double.eps <- 0.000000001
# 
# calveg <- merge(calveg_cc, calveg_cv)
# calveg <- merge(calveg, calveg_gb, tolerance=0.3)
# calveg <- merge(calveg, calveg_nce, tolerance=0.2)
# calveg <- merge(calveg, calveg_ncm, tolerance=0.3)
# calveg <- merge(calveg, calveg_ncw, tolerance=0.2)
# calveg <- merge(calveg, calveg_ni, tolerance=0.3)
# calveg <- merge(calveg, calveg_ns, tolerance=0.2)
# calveg <- merge(calveg, calveg_sc, tolerance=0.2)
# calveg <- merge(calveg, calveg_si, tolerance=0.2)
# calveg <- merge(calveg, calveg_ss, tolerance=0.2)
# 
# do.call(merge, calveg_list)

# calveg <- raster('D:/ml with cdec uf - masters/Input Data/CALVEG/calveg_raster.tif')
# 
# # find the percentage of each covertype overlayed by each basin
# 
# # extract raster values to polygons
# calveg_extracted <- extract(calveg, basins)
# 
# # cet class counts for each polygon
# calveg_extracted_counts <- lapply(calveg_extracted, table)
# 
# # calculate class percentages for each polygon
# calveg_extracted_pct <- lapply(calveg_extracted_counts, FUN=function(x){x/sum(x)})
# 
# # check if it adds to 1
# sum(calveg_extracted_pct[[4]])
# 
# # create a data.frame where missing classes are NA
# class_df <- as.data.frame(t(sapply(calveg_extracted_pct,'[',1:length(unique(calveg)))))
# 
# # replace NA's with 0 and add names
# class_df[is.na(class_df)] <- 0
# names(class_df) <- paste("class", names(class_df),sep="")
# names(class_df) <- calveg@data@attributes[[1]]$COVERTYPE
# 
# # now to percent vegetated, this includes all columns except for URB, BAR, WAT
# # URB: urban
# # BAR: baren
# # SHB: shrub
# # CON: conifers
# # HDW: hardwoods
# # WAT: water
# # MIX: mix
# # AGR: agriculture
# 
# class_df$VEGETATED <- apply(class_df[, c(3:6, 8:9)], 1, sum)
# basins@data$VEGETATED <- class_df$VEGETATED
# 
# write.csv(basins@data$VEGETATED, 'Input Data/CDEC_FNF/basins_VEGETATED.csv')

test <- read.csv('Input Data/CDEC_FNF/basins_VEGETATED.csv')
basins@data$VEGETATED <- test$x
```

# 7.0 Geology -- NRCS
Not doing this, categorical data!
```{r geo_data}
# # Geology (Reed and Bush 2005)
# # percent of basin each of nine geological classes
# # dominant geologic class in basin
# 
# nrcsgeo_ca <- shapefile('Input Data/NRCS_GEOLOGY/geology_a_ca.shp')
# nrcsgeo_nv <- shapefile('Input Data/NRCS_GEOLOGY/geology_a_nv.shp')
# nrcsgeo_or <- shapefile('Input Data/NRCS_GEOLOGY/geology_a_or.shp')
# nrcsgeo_wa <- shapefile('Input Data/NRCS_GEOLOGY/geology_a_wa.shp')
# 
# nrcsgeo_ca <- spTransform(nrcsgeo_ca, albers)
# nrcsgeo_nv <- spTransform(nrcsgeo_nv, albers)
# nrcsgeo_or <- spTransform(nrcsgeo_or, albers)
# nrcsgeo_wa <- spTransform(nrcsgeo_wa, albers)
# 
# # find the percentage of each rock type overlayed by each basin
# library(rgeos)
# basins_names <- basins@data$STATION
# rock_names <- unique(nrcsgeo_ca@data$ROCKTYPE1)
# 
# # initialize an empty data frame
# df_basins <- data.frame(matrix(0,nrow=length(basins_names),ncol=length(rock_names)))
# colnames(df_basins) <- rock_names
# rownames(df_basins) <- basins_names
# for (r in 1:(length(basins_names))){
#   h <- basins_names[r]
#   sub_basin <- basins[basins@data$STATION==h,]
#   sub_int <- intersect(nrcsgeo_ca, sub_basin)
#   sub_int@data$PROPORTIONS <- (area(sub_int)/1000000)/sub_int@data$AREASQKM
#   sub_int2 <- sub_int@data[sub_int@data$STATION==h,c("PROPORTIONS","ROCKTYPE1")]
#   sub_int2 <- aggregate(PROPORTIONS~ROCKTYPE1, data=sub_int2, FUN="sum")
#   if (nrow(sub_int2)>0){
#     for (k in 1:length(sub_int2$ROCKTYPE1)){
#       #cat("sub index number:",k,"\n")
#       c <- sub_int2$ROCKTYPE1[k]
#       col_num <- which(rock_names==c)
#       cat("df index:",r,",",col_num,"\n")
#       if (df_basins[r,col_num]!=0){
#         print("Note: this index already has a value")
#       }
#       df_basins[r,col_num] <- sub_int2[k,"PROPORTIONS"]
#     }
#   }
# }
# 
# df_basins$DOMGEOLOGY <- colnames(df_basins)[apply(df_basins,1,which.max)]
# write.csv(df_basins$DOMGEOLOGY, 'Input Data/CDEC_FNF/basins_DOMGEO.csv')

# domgeology <- read.csv('Input Data/CDEC_FNF/basins_DOMGEO.csv')
# basins@data$DOMGEOLOGY <- domgeology$x
```

# 8.0 Unimpaired Flows -- CDEC 
What: CDEC daily FNF (full natural flow) in cfs, upon further investigation these values are actually unimpaired flows
Type (extension): dataframe in r  
Time resolution: daily  
Spacial resolution: for all CDEC gauges in CA Sacramento/Delta watersheds (consisting of some DWR, USBR, PGE, ... gauges) 
Modifications: check for negative flows and NAs

```{r unimpaired_flow}
# read stations, coordinates are NAD-27, WGS-84 datum
id_list_cdec <- df$CDEC_ID

# # you can remove the discontinued basins if they are still there: c("SFR", "FTM", "BHN", "SJM") because their records sometimes do not overlap with the others, they are stations that CDEC retired
# id_list_cdec <- id_list_cdec[!id_list_cdec %in% c("BHN", "FTM", "SFR", "SJM")]

# # rewrote the sharpshootR CDECquery function on 03/27/2019  because it was broken, ignore the warnings
# CDECquery <- function(id, sensor, interval='D', start, end) {
#   # important: change the default behavior of data.frame
#   opt.original <- options(stringsAsFactors = FALSE)
# 
#   # sanity-check:
#   if(missing(id) | missing(sensor) | missing(start) | missing(end))
#     stop('missing arguments', call.=FALSE)
# 
#   # changes made in u
#   # construct the URL for the DWR website
#   u <- paste0(
#     'https://cdec.water.ca.gov/dynamicapp/req/CSVDataServlet?Stations=', id,
#     '&sensor_num=', sensor,
#     '&dur_code=', interval,
#     '&start_date=', start,
#     '&end_date=', end)
#     #'&data_wish=Download CSV Data Now')
# 
#   # encode as needed
#   u <- URLencode(u)
# 
#   # init temp file and download
#   tf <- tempfile()
#   suppressWarnings(download.file(url=u, destfile=tf, quiet=TRUE))
# 
#   # changes made in colClasses
#   # try to parse CSV
#   # STATION_ID,DURATION,SENSOR_NUMBER,SENSOR_TYPE,DATE TIME,OBS DATE,VALUE,DATA_FLAG,UNITS
#   # BND,D,8,FNF,19990501 0000,19990501 0000,16260, ,CFS
# 
#   d <- try(read.csv(file=tf, header=TRUE, quote="'", na.strings='---', stringsAsFactors=FALSE, colClasses=c('character', 'character', 'numeric', 'character', 'character', 'character', 'numeric', 'character', 'character')),  silent=TRUE)
# 
#   # catch errors
#   if(class(d) == 'try-error') {
#     ref.url <- paste0('invalid URL; see ','https://cdec.water.ca.gov/dynamicapp/req/CSVDataServlet?Stations=', id)
#     stop(ref.url, call.=FALSE)
#   }
# 
#   # no data available
#   if(nrow(d) == 0)
#     stop('query returned no data', call.=FALSE)
# 
#   # changes made in dataframe and formatting
#   # convert date/time to R-friendly format
#   d$datetime <- as.POSIXct(d$DATE.TIME, format="%Y%m%d %H%M")
#   d$datetime <- as.Date(substr(d$datetime, 1, 10), format="%Y-%m-%d")
# 
#   # strip-out extras and format the columns in datetime, value, CDEC_ID
#   c <- data.frame(cbind.data.frame(DATE=d$datetime, CDEC_ID=d$STATION_ID, SENSOR=d$SENSOR_NUMBER, FLOW=as.numeric(d$VALUE)))
# 
#   # return the result in a more useful order
#   return(c)
# }
# 
# mflowlist_cdec <- list()
# for (id in id_list_cdec){
#   newdata <- CDECquery(id, sensor=8, interval="D", start="1982-10-01", end="2018-12-01")
#   newdata <- newdata[newdata$SENSOR==8,]
#   # mflowlist_cdec[[id]] <- aggregate(FLOW ~ DATE, data=newdata, FUN=mean, na.rm=TRUE, na.action=NULL)
#   mflowlist_cdec[[id]] <- newdata
#   # mflowlist_cdec[[id]]$CDEC_ID <- id
# }
# 
# # coerce list into a data frame
# mflowdf_cdec <- do.call("rbind", mflowlist_cdec)
# 
# # get rid of the sensor number info
# mflowdf_cdec <- mflowdf_cdec[,c("DATE", "CDEC_ID", "FLOW")]
# 
# # write to a csv file
# write.csv(mflowdf_cdec, file="Input Data/CDEC_FNF/cdec_fnf_autodl.csv", row.names=FALSE)

cdec_fnf_autodl <- read.csv("Input Data/CDEC_FNF/cdec_fnf_autodl.csv")

cdec_fnf_long <- cdec_fnf_autodl

# fix the column formats, apparently it is important for dcast and melting
cdec_fnf_long$DATE <-  as.Date(cdec_fnf_long$DATE, format="%Y-%m-%d")
cdec_fnf_long$CDEC_ID <- as.factor(cdec_fnf_long$CDEC_ID)
str(cdec_fnf_long)

# make wide format data too
cdec_fnf_wide <- dcast(melt(cdec_fnf_long, id.vars=c("DATE", "CDEC_ID")), DATE~CDEC_ID)

# some dates are missing for WHI, subset the wide format data by time of interest and turn back into long format, so the missing dates are now NAs instead of missing
cdec_fnf_wide <- cdec_fnf_wide[cdec_fnf_wide$DATE>="2000-10-01" & cdec_fnf_wide$DATE<="2015-06-30", ]
cdec_fnf_long <- melt(cdec_fnf_wide, id.vars="DATE", value.name="FLOW", variable.name="CDEC_ID")

# write it out for other parts of the project (i.e., post processing)
write.csv(cdec_fnf_wide, "Intermediary Data/cdec_fnf_wide.csv", row.names=FALSE)
write.csv(cdec_fnf_long, "Intermediary Data/cdec_fnf_long.csv", row.names=FALSE)
```

# 9.0 Month & Season & Year
```{r month_season_year}
# fix the date
cdec_fnf <- cdec_fnf_long
cdec_fnf$DATE <- as.Date(cdec_fnf$DATE, "%Y-%m-%d")
cdec_fnf$MONTH <- month.abb[as.integer(substring(cdec_fnf$DATE, 6, 7))]
cdec_fnf$MONTH <- as.factor(cdec_fnf$MONTH)

# make a variable that captures the distance from October (the start of the new water year)
# this is messing up the order
ordinal <- c(4,5,6,7,6,5,4,3,2,1,2,3)
key <- data.frame(MONTH_ORDINAL=ordinal, mat=month.abb)
cdec_fnf <- merge(cdec_fnf, key, by.x = "MONTH", by.y = "mat")

# make a season finding function
getseason <- function(dates) {
    WS <- as.Date("2012-12-15", format = "%Y-%m-%d") # Winter Solstice
    SE <- as.Date("2012-3-15",  format = "%Y-%m-%d") # Spring Equinox
    SS <- as.Date("2012-6-15",  format = "%Y-%m-%d") # Summer Solstice
    FE <- as.Date("2012-9-15",  format = "%Y-%m-%d") # Fall Equinox

    # Convert dates from any year to 2012 dates
    d <- as.Date(strftime(dates, format="2012-%m-%d"))

    ifelse (d >= WS | d < SE, "Winter",
      ifelse (d >= SE & d < SS, "Spring",
        ifelse (d >= SS & d < FE, "Summer", "Fall")))
}

cdec_fnf$SEASON <- as.factor(getseason(cdec_fnf$DATE))
cdec_fnf$YEAR <- as.numeric(substring(cdec_fnf$DATE, 1, 4))
cdec_fnf$WATERYEAR <- wateryear(cdec_fnf$DATE, startmonth=10)

# order
cdec_fnf <- cdec_fnf[order(cdec_fnf$DATE), ]
cdec_fnf <- cdec_fnf[order(cdec_fnf$CDEC_ID), ]
```

# 10.0 Data Prep for modelling  
```{r bind_data}
df$AREASQM <- basins@data$AREASQM 
df$SHAPE <- basins@data$SHAPE
df$COMPACTNESS <- basins@data$COMPACTNESS
df$MEANELEV <- basins@data$MEANELEV
df$BASINRELIEFRATIO <- basins@data$BASINRELIEFRATIO
df$KSAT <- basins@data$KSAT
df$SILT <- basins@data$SILT
df$SAND <- basins@data$SAND
df$CLAY <- basins@data$CLAY
df$AWC <- basins@data$AWC
df$LAMBDA <- basins@data$LAMBDA
df$N <- basins@data$N
df$RESDT <- basins@data$RESDT
df$VEGETATED <- basins@data$VEGETATED
# df$DOMGEOLOGY <- basins@data$DOMGEOLOGY

cdec_fnf_merge <- merge(cdec_fnf, df, by="CDEC_ID")

moddf <- merge(cdec_fnf_merge, tmp_long, by=c("DATE","CDEC_ID"))
moddf <- merge(moddf, tmplag1_long, by=c("DATE","CDEC_ID"))
moddf <- merge(moddf, tmplag2_long, by=c("DATE","CDEC_ID"))
moddf <- merge(moddf, tmplag3_long, by=c("DATE","CDEC_ID"))
moddf <- merge(moddf, ppt_long, by=c("DATE","CDEC_ID"))
moddf <- merge(moddf, pptlag1_long, by=c("DATE","CDEC_ID"))
moddf <- merge(moddf, pptlag2_long, by=c("DATE","CDEC_ID"))
moddf <- merge(moddf, pptlag3_long, by=c("DATE","CDEC_ID"))
moddf <- merge(moddf, snowcumul_df_long, by=c("DATE","CDEC_ID"))

# Here is what's in the dataframe
dput(names(moddf))

# order the columns somewhat logically
moddf <- moddf[, c("DATE", "CDEC_ID", "STATION", "RIVER_BASIN", "COUNTY", "OPERATOR", "LONGITUDE", "LATITUDE", "MONTH", "MONTH_ORDINAL", "SEASON", "YEAR", "WATERYEAR", "TMP", "TMPLAG1", "TMPLAG2", "TMPLAG3", "PPT", "PPTLAG1", "PPTLAG2", "PPTLAG3", "SNOW", "AREASQM", "SHAPE", "COMPACTNESS", "MEANELEV", "BASINRELIEFRATIO", "KSAT", "SILT", "SAND", "CLAY", "AWC", "LAMBDA", "N", "RESDT", "VEGETATED", "FLOW")]
```

```{r na_neg_flows}
# first, let's plot where the NAs and negative flows are happenning
df_nas <- moddf[is.na(moddf$FLOW),]
df_negf <- moddf[moddf$FLOW<0, ]

# plot 
df_nas_count <- aggregate(VEGETATED~CDEC_ID, df_nas, FUN="length")
df_negf_count <- aggregate(VEGETATED~CDEC_ID, df_negf, FUN="length")

library(naniar)

cbpwhite <-  c("#FFFFFF", "#CC79A7",  "#E69F00", "#F0E442", "#009E73", "#0072B2", "#D55E00", "#56B4E9")
for(i in seq_along(df_negf_count$CDEC_ID)){
  h <- df_negf_count$CDEC_ID[i]
  png(paste0('Output Data/flow_vis/rplot11_flows_',h ,'.png'), width=6.5, height=4, units="in", pointsize=8, res=300)
    plottoprint <- ggplot(moddf[moddf$CDEC_ID==h,], aes(x = DATE, y = FLOW)) +
    geom_miss_point(alpha=0.5) +
    geom_point(aes(color=ifelse(FLOW>0, "Unimpaired Flows", "Negative Flows")), alpha=0.5) +
    scale_colour_manual(breaks= c("Unimpaired Flows", "Not Missing", "Negative Flows", "Missing"), labels = c("Unimpaired Flows", "Not Missing", "Negative Flows", "Missing"), values=cbpwhite[c(8, 1, 7, 3)])+
    labs(color="", x="", y="Unimpaired Flow (CFS)") +
    theme_bw()
    print(plottoprint)
  dev.off()
}
  
# get rid of NAs, make them 0
moddf$FLOW <- ifelse(is.na(moddf$FLOW), 0, moddf$FLOW)

# get rid of negative flows, make them 0
moddf$FLOW <- ifelse(moddf$FLOW<0, 0, moddf$FLOW)
```

```{r final_touches}
summary(moddf)

# are they all the same size? WHI was missing observations...
aggregate(DATE ~ CDEC_ID , data = moddf, FUN = "length")

# order by basin name 
moddf <- moddf[order(moddf$CDEC_ID), ]
row.names(moddf) <- 1:nrow(moddf)

# clean up column types if needed
str(moddf)

# output the dataframe to a .rds. use this for larger data 
saveRDS(moddf, file="Intermediary Data/moddf.rds" )

# now just read it back in
moddf <- readRDS("Intermediary Data/moddf.rds")

# check a few things
plot(moddf$PPT, moddf$FLOW)
plot(moddf$PPT, moddf$PPTLAG1)
```




